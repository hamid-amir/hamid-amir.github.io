<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Hamidreza Amirzadeh </title> <meta name="author" content="Hamidreza Amirzadeh"> <meta name="description" content=""> <meta name="keywords" content="mechanistic-interpretability, ai-safety, academic-website, hamidreza-amirzadeh"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@/css/mdb.min.css" integrity="" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/prof_pic.jpg?6e19f5ae26e1834e29bdee99789502e8"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hamid-amir.github.io/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About Me <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Hamidreza</span> Amirzadeh </h1> <p class="desc">Mechanistic Interpretability Researcher | Trustworthy AI Proponent | Master's Student at <a href="https://en.sharif.edu/" rel="external nofollow noopener" target="_blank">Sharif University of Technology</a>.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/prof_pic.jpg?6e19f5ae26e1834e29bdee99789502e8" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>Speech and Language Processing Lab (SLPL)</p> <p>Sharif University of Technology</p> <p>Tehran, Iran</p> </div> </div> <div class="clearfix"> <h3 id="hello-im-hamidreza-">Hello! Iâ€™m Hamidreza ðŸ˜Š</h3> <p>I am a Masterâ€™s student in the <a href="https://ce.sharif.edu/" rel="external nofollow noopener" target="_blank">Computer Engineering Department</a> at <a href="https://en.sharif.edu/" rel="external nofollow noopener" target="_blank">Sharif University of Technology (SUT)</a>, where I am dedicated to enhancing AI safety through mechanistic interpretability. My research focuses on improving the transparency and interpretability of transformer language models, supporting the broader objective of achieving AI alignment.</p> <p>As a Research Assistant at the <a href="https://www.sharif.edu/web/slpl_ce" rel="external nofollow noopener" target="_blank">Speech and Language Processing Lab</a> under the supervision of <a href="https://scholar.google.com/citations?user=ebEhWZwAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">Prof. Sameti</a>, I am broadly interested in AI, Machine Learning, and Natural Language Processing (NLP). I am particularly focused on model interpretability and the development of trustworthy AI systems.</p> <p>Prior to joining SUT, I completed my Bachelorâ€™s degree in Mechanical Engineering at <a href="https://aut.ac.ir/en" rel="external nofollow noopener" target="_blank">Tehran Polytechnic (AUT)</a>. During my undergraduate studies, I served as a Research Assistant in the New Technologies and Robotics Lab under the guidance of <a href="https://scholar.google.com/citations?user=xx8zwXYAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">Dr. Zareinejad</a>.</p> <p>I am passionate about advancing the field of AI by making models more interpretable, transparent, and trustworthy, ultimately contributing to a safer and more aligned AI future.</p> <h3 id="research-focus">Research Focus</h3> <p>In the early stages of my mechanistic interpretability research, I began my journey by concentrating on the following areas:</p> <ul> <li>Investigating how different LMs utilize multiple cue words to predict grammatical tasks, employing <a href="https://arxiv.org/abs/2301.12971" rel="external nofollow noopener" target="_blank">context mixing</a> techniques to examine the internal representations, and using <a href="https://arxiv.org/abs/2202.05262" rel="external nofollow noopener" target="_blank">activation patching</a> to establish a causal link between these representations and the modelâ€™s predictions.</li> <li>Implementing <a href="http://arxiv.org/abs/2309.08600" rel="external nofollow noopener" target="_blank">Sparse Autoencoders</a> to conquer the <a href="https://transformer-circuits.pub/2023/superposition-composition/index.html." rel="external nofollow noopener" target="_blank">Superposition</a>, i.e. finding the redemption in Neural nets interpretability!</li> <li>Aiming to extract reasoning circuits within a zero-shot or chain-of-thought reasoning process of a large language model (LLM).</li> </ul> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="taghavi2023imaginations" class="col-sm-8"> <div class="title">Imaginations of WALL-E : Reconstructing Experiences with an Imagination-Inspired Module for Advanced AI Systems</div> <div class="author"> Zeinab Sadat Taghavi ,Â Soroush Gooran ,Â Seyed Arshan Dalili ,Â <em>Hamidreza Amirzadeh</em>,Â Mohammad Jalal Nematbakhsh ,Â andÂ Hossein Sameti </div> <div class="periodical"> <em>arXiv preprint arXiv:2308.10354</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2308.10354" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In this paper, we introduce a novel Artificial Intelligence (AI) system inspired by the philosophical and psychoanalytical concept of imagination as a â€œRe-construction of Experiences". Our AI system is equipped with an imagination-inspired module that bridges the gap between textual inputs and other modalities, enriching the derived information based on previously learned experiences. A unique feature of our system is its ability to formulate independent perceptions of inputs. This leads to unique interpretations of a concept that may differ from human interpretations but are equally valid, a phenomenon we term as â€œInterpretable Misunderstanding". We employ large-scale models, specifically a Multimodal Large Language Model (MLLM), enabling our proposed system to extract meaningful information across modalities while primarily remaining unimodal. We evaluated our system against other large language models across multiple tasks, including emotion recognition and question-answering, using a zero-shot methodology to ensure an unbiased scenario that may happen by fine-tuning. Significantly, our system outperformed the best Large Language Models (LLM) on the MELD, IEMOCAP, and CoQA datasets, achieving Weighted F1 (WF1) scores of 46.74%, 25.23%, and Overall F1 (OF1) score of 17%, respectively, compared to 22.89%, 12.28%, and 7% from the well-performing LLM. The goal is to go beyond the statistical view of language processing and tie it to human concepts such as philosophy and psychoanalysis. This work represents a significant advancement in the development of imagination-inspired AI systems, opening new possibilities for AI to generate deep and interpretable information across modalities, thereby enhancing human-AI interaction.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">taghavi2023imaginations</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Imaginations of WALL-E : Reconstructing Experiences with an Imagination-Inspired Module for Advanced AI Systems}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Taghavi, Zeinab Sadat and Gooran, Soroush and Dalili, Seyed Arshan and Amirzadeh, Hamidreza and Nematbakhsh, Mohammad Jalal and Sameti, Hossein}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2308.10354}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2308.10354}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.AI}</span><span class="p">,</span>
  <span class="na">thumbnail</span> <span class="p">=</span> <span class="s">{https://github.com/hamid-amir/hamid-amir.github.io/blob/master/assets/img/prof_pic.jpg}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="rahimi-etal-2024-hallusafe" class="col-sm-8"> <div class="title">HalluSafe at SemEval-2024 Task 6: An NLI-based Approach to Make LLMs Safer by Better Detecting Hallucinations and Overgeneration Mistakes</div> <div class="author"> Zahra Rahimi ,Â <em>Hamidreza Amirzadeh</em>,Â Alireza Sohrabi ,Â Zeinab Taghavi ,Â andÂ Hossein Sameti </div> <div class="periodical"> <em>In Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)</em> , Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2024.semeval-1.22.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>The advancement of large language models (LLMs), their ability to produce eloquent and fluent content, and their vast knowledge have resulted in their usage in various tasks and applications. Despite generating fluent content, this content can contain fabricated or false information. This problem is known as hallucination and has reduced the confidence in the output of LLMs. In this work, we have used Natural Language Inference to train classifiers for hallucination detection to tackle SemEval-2024 Task 6-SHROOM (Mickus et al., 2024) which is defined in three sub-tasks: Paraphrase Generation, Machine Translation, and Definition Modeling. We have also conducted experiments on LLMs to evaluate their ability to detect hallucinated outputs. We have achieved 75.93% and 78.33% accuracy for the modelaware and model-agnostic tracks, respectively. The shared links of our models and the codes are available on GitHub.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rahimi-etal-2024-hallusafe</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{H}allu{S}afe at {S}em{E}val-2024 Task 6: An {NLI}-based Approach to Make {LLM}s Safer by Better Detecting Hallucinations and Overgeneration Mistakes}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rahimi, Zahra and Amirzadeh, Hamidreza and Sohrabi, Alireza and Taghavi, Zeinab and Sameti, Hossein}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Ojha, Atul Kr. and Do{\u{g}}ru{\"o}z, A. Seza and Tayyar Madabushi, Harish and Da San Martino, Giovanni and Rosenthal, Sara and Ros{\'a}, Aiala}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Mexico City, Mexico}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2024.semeval-1.22}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2024.semeval-1.22}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{139--147}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="amirzadeh2024data2lang2vecdatadriventypological" class="col-sm-8"> <div class="title">data2lang2vec: Data Driven Typological Features Completion</div> <div class="author"> <em>Hamidreza Amirzadeh</em>,Â Sadegh Jafari ,Â Anika Harju ,Â andÂ Rob Goot </div> <div class="periodical"> Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2409.17373" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Language typology databases enhance multi-lingual Natural Language Processing (NLP) by improving model adaptability to diverse linguistic structures. The widely-used lang2vec toolkit integrates several such databases, but its coverage remains limited at 28.9%. Previous work on automatically increasing coverage predicts missing values based on features from other languages or focuses on single features, we propose to use textual data for better-informed feature prediction. To this end, we introduce a multi-lingual Part-of-Speech (POS) tagger, achieving over 70% accuracy across 1,749 languages, and experiment with external statistical features and a variety of machine learning algorithms. We also introduce a more realistic evaluation setup, focusing on likely to be missing typology features, and show that our approach outperforms previous work in both setups.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">amirzadeh2024data2lang2vecdatadriventypological</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{data2lang2vec: Data Driven Typological Features Completion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Amirzadeh, Hamidreza and Jafari, Sadegh and Harju, Anika and van der Goot, Rob}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2409.17373}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2409.17373}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="amirzadeh2024how" class="col-sm-8"> <div class="title">How Language Models Prioritize Contextual Grammatical Cues?</div> <div class="author"> <em>Hamidreza Amirzadeh</em>,Â Afra Alishahi ,Â andÂ Hosein Mohebbi </div> <div class="periodical"> <em>In The 7th BlackboxNLP Workshop at EMNLP 2024</em> , Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/pdf?id=XDxNnRHGRp" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Transformer-based language models have shown an excellent ability to effectively capture and utilize contextual information. Although various analysis techniques have been used to quantify and trace the contribution of single contextual cues to a target task such as subject-verb agreement or coreference resolution, scenarios in which multiple relevant cues are available in the context remain underexplored. In this paper, we investigate how language models handle gender agreement when multiple gender cue words are present, each capable of independently disambiguating a target gender pronoun. We analyze two widely used Transformer-based models: BERT, an encoder-based, and GPT-2, a decoder-based model. Our analysis employs two complementary approaches: context mixing analysis, which tracks information flow within the model, and a variant of activation patching, which measures the impact of cues on the modelâ€™s prediction. We find that BERT tends to prioritize the first cue in the context to form both the target word representations and the modelâ€™s prediction, while GPT-2 relies more on the final cue. Our findings reveal striking differences in how encoder-based and decoder-based models prioritize and use contextual information for their predictions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">amirzadeh2024how</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{How Language Models Prioritize Contextual Grammatical Cues?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Amirzadeh, Hamidreza and Alishahi, Afra and Mohebbi, Hosein}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The 7th BlackboxNLP Workshop at EMNLP 2024}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=XDxNnRHGRp}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%68.%61%6D%69%72%7A%61%64%65%68%31%33%37%38@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=p5OmQIwAAAAJ&amp;hl" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/hamid-amir" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/HamidrezaAmirzadeh" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/Hamiiiidreza99" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://www.zotero.org/hamiiidreza" title="Zotero" rel="external nofollow noopener" target="_blank"><i class="ai ai-zotero"></i></a> </div> <div class="contact-note">I'm always excited to connect with new people, especially those who share similar interests!</div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2024 Hamidreza Amirzadeh. Last updated: October 12, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@/dist/jquery.min.js" integrity="" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@/js/mdb.min.js" integrity="" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@/dist/masonry.pkgd.min.js" integrity="" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@/imagesloaded.pkgd.min.js" integrity="" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@/dist/medium-zoom.min.js" integrity="" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>