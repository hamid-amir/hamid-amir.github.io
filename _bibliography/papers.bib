@article{taghavi2023imaginations,
      bibtex_show={true},
      title={Imaginations of WALL-E : Reconstructing Experiences with an Imagination-Inspired Module for Advanced AI Systems}, 
      author={Zeinab Sadat Taghavi and Soroush Gooran and Seyed Arshan Dalili and Hamidreza Amirzadeh and Mohammad Jalal Nematbakhsh and Hossein Sameti},
      year={2023},
      journal={arXiv preprint arXiv:2308.10354},
      eprint={2308.10354},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@inproceedings{rahimi-etal-2024-hallusafe,
    title = "{H}allu{S}afe at {S}em{E}val-2024 Task 6: An {NLI}-based Approach to Make {LLM}s Safer by Better Detecting Hallucinations and Overgeneration Mistakes",
    author = "Rahimi, Zahra  and
      Amirzadeh, Hamidreza  and
      Sohrabi, Alireza  and
      Taghavi, Zeinab  and
      Sameti, Hossein",
    editor = {Ojha, Atul Kr.  and
      Do{\u{g}}ru{\"o}z, A. Seza  and
      Tayyar Madabushi, Harish  and
      Da San Martino, Giovanni  and
      Rosenthal, Sara  and
      Ros{\'a}, Aiala},
    booktitle = "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.semeval-1.22",
    doi = "10.18653/v1/2024.semeval-1.22",
    pages = "139--147",
    abstract = "The advancement of large language models (LLMs), their ability to produce eloquent and fluent content, and their vast knowledge have resulted in their usage in various tasks and applications. Despite generating fluent content, this content can contain fabricated or false information. This problem is known as hallucination and has reduced the confidence in the output of LLMs. In this work, we have used Natural Language Inference to train classifiers for hallucination detection to tackle SemEval-2024 Task 6-SHROOM (Mickus et al., 2024) which is defined in three sub-tasks: Paraphrase Generation, Machine Translation, and Definition Modeling. We have also conducted experiments on LLMs to evaluate their ability to detect hallucinated outputs. We have achieved 75.93{\%} and 78.33{\%} accuracy for the modelaware and model-agnostic tracks, respectively. The shared links of our models and the codes are available on GitHub.",
}

@inproceedings{Amirzadeh2024data2lang2vecDD,
  title={data2lang2vec: Data Driven Typological Features Completion},
  author={Hamidreza Amirzadeh and Sadegh Jafari and Anika Harju and Rob van der Goot},
  year={2024},
  selected = {true},
  url={https://api.semanticscholar.org/CorpusID:272910792}
}

@inproceedings{
amirzadeh2024how,
title={How Language Models Prioritize Contextual Grammatical Cues?},
author={Hamidreza Amirzadeh and Afra Alishahi and Hosein Mohebbi},
booktitle={The 7th BlackboxNLP Workshop},
year={2024},
selected = {true},
url={https://openreview.net/forum?id=XDxNnRHGRp}
}
